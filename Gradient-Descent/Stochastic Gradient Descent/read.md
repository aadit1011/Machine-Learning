<p>Stochastic Gradient Descent::
Stochastic Gradient Descent (SGD) is a variant where the gradient is computed for a single sample (or a small mini-batch) at a time rather than the whole dataset. Full-batch GD uses all samples to compute the gradient, yielding a smooth, consistent update.SGD uses one sample at a time. This introduces randomness, so each update is noisy and may vary. Because of this noise, the path to the optimum can fluctuate; however, on average, the algorithm converges.
</p>

<h1>Author-Aadit Sharma Shiwakoti</h1>
